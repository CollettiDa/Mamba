{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bbfe9c9d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<module 'my_mambanet' from '/home/collettida/myprojects/ExploringMamba/code/my_mambanet.py'>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# import mamba_ssm\n",
    "import networks.mamba_sys as mamba_sys\n",
    "import torch\n",
    "import torch as tc\n",
    "import my_mambanet\n",
    "import importlib\n",
    "import torch.nn as nn\n",
    "\n",
    "from einops import rearrange, repeat\n",
    "\n",
    "importlib.reload(my_mambanet)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "347b87aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([16, 7, 7, 96])\n",
      "torch.Size([16, 8, 8, 96])\n"
     ]
    }
   ],
   "source": [
    "class PatchEmbed2D(nn.Module):\n",
    "    r\"\"\" Image to Patch Embedding\n",
    "    Args:\n",
    "        patch_size (int): Patch token size. Default: 4.\n",
    "        in_chans (int): Number of input image channels. Default: 3.\n",
    "        embed_dim (int): Number of linear projection output channels. Default: 96.\n",
    "        norm_layer (nn.Module, optional): Normalization layer. Default: None\n",
    "    \"\"\"\n",
    "    def __init__(self, img_size=224, patch_size=4, in_chans=3, embed_dim=96, groups=1, norm_layer=None, **kwargs):\n",
    "        super().__init__()\n",
    "        if isinstance(patch_size, int):\n",
    "            patch_size = (patch_size, patch_size)\n",
    "        self.proj = nn.Conv2d(in_chans, \n",
    "                              embed_dim, \n",
    "                              kernel_size=patch_size,\n",
    "                              stride=patch_size,\n",
    "                              groups=groups)\n",
    "        if norm_layer is not None:\n",
    "            self.norm = norm_layer(embed_dim)\n",
    "        else:\n",
    "            self.norm = None\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.proj(x)\n",
    "        m = nn.ZeroPad2d((0, 1, 0, 1))\n",
    "        # x = m(x)\n",
    "        x = x.permute(0, 2, 3, 1)\n",
    "        if self.norm is not None:\n",
    "            x = self.norm(x)\n",
    "        return x\n",
    "\n",
    "inp = torch.randn(16,30,21,21).cuda()\n",
    "m = nn.ZeroPad2d((0, 3, 0, 3))\n",
    "inp_p = m(inp)\n",
    "# int = torch.randn(16,1,21,21).cuda()\n",
    "pb2d = PatchEmbed2D(img_size=21, patch_size=3, in_chans=30, embed_dim=96, groups=1).cuda()\n",
    "out = pb2d(inp)\n",
    "print(out.shape)\n",
    "out = pb2d(inp_p)\n",
    "print(out.shape)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "51f28461",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([16, 16, 64, 96])\n"
     ]
    }
   ],
   "source": [
    "class PatchEmbedVideo(nn.Module):\n",
    "    \"\"\" Video to Patch Embedding\n",
    "    Args:\n",
    "        img_size (int): Frame size.  Default: 21\n",
    "        in_chans (int): Number of input image channels. Default: 3\n",
    "        n_frames (int): Number of frames. Default: 8\n",
    "        patch_size (int): Patch token size. Default: 3\n",
    "        stride (int): Stride of the patch embedding. Default: None\n",
    "        embed_dim (int): Number of linear projection output channels. Default: 96.\n",
    "        norm_layer (nn.Module, optional): Normalization layer. Default: None\n",
    "    \"\"\"\n",
    "    def __init__(self, image_size=21, n_frames=8, patch_size=3, \n",
    "                 stride=None, in_chans=2, embed_dim=96, groups=1, \n",
    "                 norm_layer=None, **kwargs):\n",
    "        super().__init__()\n",
    "        if isinstance(patch_size, int):\n",
    "            patch_size = (patch_size, patch_size)\n",
    "        if stride is None:\n",
    "            stride = patch_size\n",
    "        if isinstance(stride, int):\n",
    "            stride = (stride, stride)\n",
    "\n",
    "        self.proj = nn.Conv2d(in_chans, \n",
    "                              embed_dim, \n",
    "                              kernel_size=patch_size,\n",
    "                              stride=patch_size,\n",
    "                              groups=groups)\n",
    "        if norm_layer is not None:\n",
    "            self.norm = norm_layer(embed_dim)\n",
    "        else:\n",
    "            self.norm = None \n",
    "        \n",
    "    def forward(self, x: tc.Tensor):\n",
    "        \"\"\" Forward function.\n",
    "        Args:\n",
    "            x: (tc.Tensor) Input video of shape (B, T, C, H, W)\n",
    "        Returns:\n",
    "            tc.Tensor: Patch embedded video of shape (B, H', W', embed_dim)\n",
    "        \"\"\"\n",
    "        B, T, C, H, W = x.shape\n",
    "         \n",
    "        for t in range(T):\n",
    "            x_t = x[:, t, :, :, :]  # (B, C, H, W)\n",
    "            x_t = self.proj(x_t)  # (B, embed_dim, H', W')\n",
    "            if t == 0:\n",
    "                x_out = x_t.unsqueeze(1)  # (B, 1, embed_dim, H', W')\n",
    "            else:\n",
    "                x_out = tc.cat((x_out, x_t.unsqueeze(1)), dim=1)  # (B, T, embed_dim, H', W')\n",
    "        \n",
    "        \n",
    "        x_out = tc.reshape(x_out, (B, T, x_out.shape[2], x_out.shape[3]*x_out.shape[4]))\n",
    "        x_out = x_out.permute(0, 1, 3, 2)  # (B, T, H'*W', embed_dim)\n",
    "        if self.norm is not None:\n",
    "            x_out = self.norm(x_out)\n",
    "        \n",
    "        return x_out\n",
    "\n",
    "inp = torch.randn(16,16,2,24,24).cuda()\n",
    "m = nn.ZeroPad2d((0, 3, 0, 3))\n",
    "# inp_p = m(inp)\n",
    "# int = torch.randn(16,1,21,21).cuda()\n",
    "pbv = PatchEmbedVideo().cuda()\n",
    "out = pbv(inp)\n",
    "print(out.shape)\n",
    "# out = pbv(inp_p)\n",
    "# print(out.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c4c165e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([16, 4, 4, 192])\n"
     ]
    }
   ],
   "source": [
    "class PatchMerging2D(nn.Module):\n",
    "    r\"\"\" PatchMerging2D performs spatial downsampling by a factor of 2.\n",
    "        It groups each 2x2 neighborhood of tokens, concatenates their channels\n",
    "        (C → 4C), applies LayerNorm, and projects them to a lower-dimensional\n",
    "        embedding (4C → 2C). The output has half the spatial resolution and\n",
    "        twice the channel dimension: (B, H, W, C) → (B, H/2, W/2, 2C).\n",
    "        Patch Merging Layer.\n",
    "    Args:\n",
    "        dim (int): Resolution of input token.\n",
    "        norm_layer (nn.Module, optional): Normalization layer.  Default: nn.LayerNorm\n",
    "    \"\"\"\n",
    "    def __init__(self, dim, norm_layer=nn.LayerNorm):\n",
    "        super().__init__()\n",
    "        self.dim = dim\n",
    "        self.reduction = nn.Linear(4 * dim, 2 * dim, bias=False)\n",
    "        self.norm = norm_layer(4 * dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        B, H, W, C = x.shape\n",
    "\n",
    "        SHAPE_FIX = [-1, -1]\n",
    "        if H % 2 != 0:\n",
    "            print(f\"Warning, x.shape {x.shape} is not match even ===========\", flush=True)\n",
    "            SHAPE_FIX[0] = H // 2\n",
    "            SHAPE_FIX[1] = W // 2\n",
    "        x0 = x[:, 0::2, 0::2, :]  # B H/2 W/2 C\n",
    "        x1 = x[:, 1::2, 0::2, :]  # B H/2 W/2 C\n",
    "        x2 = x[:, 0::2, 1::2, :]  # B H/2 W/2 C\n",
    "        x3 = x[:, 1::2, 1::2, :]  # B H/2 W/2 C\n",
    "        \n",
    "        if SHAPE_FIX[0] > 0:\n",
    "            x0 = x0[:, :SHAPE_FIX[0], :SHAPE_FIX[1], :]\n",
    "            x1 = x1[:, :SHAPE_FIX[0], :SHAPE_FIX[1], :]\n",
    "            x2 = x2[:, :SHAPE_FIX[0], :SHAPE_FIX[1], :]\n",
    "            x3 = x3[:, :SHAPE_FIX[0], :SHAPE_FIX[1], :]\n",
    "\n",
    "        x = tc.cat([x0, x1, x2, x3], -1)  # B H/2 W/2 4*C\n",
    "        x = x.view(B, H//2, W//2, 4 * C)  # B H/2*W/2 4*C\n",
    "\n",
    "        x = self.norm(x)\n",
    "        x = self.reduction(x)       # FC layer to reduce dimension\n",
    "\n",
    "        return x\n",
    "\n",
    "pm2d = PatchMerging2D(dim=96).cuda()\n",
    "inp2 = torch.randn(16,8,8,96).cuda()\n",
    "out2 = pm2d(inp2)\n",
    "print(out2.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ec59d25c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([16, 8, 8, 96])\n"
     ]
    }
   ],
   "source": [
    "class PatchExpand(nn.Module):\n",
    "    \"\"\"\n",
    "    PatchExpand layer.\n",
    "\n",
    "    Upsamples the input feature map by a factor of 2 by converting channel\n",
    "    information into spatial resolution. The operation applies a linear\n",
    "    projection followed by a PixelShuffle-style rearrangement:\n",
    "    (B, H, W, C) → (B, 2H, 2W, C/2).\n",
    "    \"\"\"\n",
    "    def __init__(self, dim, dim_scale=2, norm_layer=nn.LayerNorm):\n",
    "        super().__init__()\n",
    "        self.dim = dim\n",
    "        self.dim_scale = dim_scale\n",
    "        self.expand = nn.Linear(\n",
    "            dim,  2*dim, bias = False) if dim_scale == 2 else nn.Identity()\n",
    "            # applied to last dimension\n",
    "        self.norm = norm_layer(dim // dim_scale)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.expand(x)      # B, H, W, C -> B, H, W, 2C\n",
    "        B, H, W, C = x.shape\n",
    "        x = rearrange(x, 'b h w (p1 p2 c) -> b (h p1) (w p2) c', p1=2, p2=2, c=C//4)\n",
    "        x = self.norm(x)\n",
    "        return x\n",
    "\n",
    "pe = PatchExpand(dim=96*2).cuda()\n",
    "inp3 = torch.randn(16,4,4,192).cuda()\n",
    "out3 = pe(inp3)\n",
    "print(out3.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d5fa6357",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "out4.shape = torch.Size([16, 1, 64, 288])\n"
     ]
    }
   ],
   "source": [
    "class FinalPatchExpand(nn.Module):\n",
    "    \"\"\"\n",
    "    FinalPatchExpand layer.\n",
    "    # a kind of unembedding\n",
    "    Upsamples the input feature map by a factor of 4 by converting channel\n",
    "    information into spatial resolution. The operation applies a linear\n",
    "    projection followed by a PixelShuffle-style rearrangement:\n",
    "    (B, H, W, C) → (B, 4H, 4W, C).\n",
    "    \"\"\"\n",
    "    def __init__(self, dim, dim_scale=4, norm_layer=nn.LayerNorm):\n",
    "        super().__init__()\n",
    "        self.dim = dim\n",
    "        self.dim_scale = dim_scale\n",
    "        self.expand = nn.Linear(dim, self.dim_scale**2 * dim, bias = False)\n",
    "            # applied to last dimension\n",
    "        self.output_dim = dim            \n",
    "        self.norm = norm_layer(self.output_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.expand(x)\n",
    "        B, H, W, C = x.shape\n",
    "        x = rearrange(x, 'b h w (p1 p2 c) -> b (h p1) (w p2) c', p1=self.dim_scale, p2=self.dim_scale, c=C//self.dim_scale**2)\n",
    "        x = self.norm(x)\n",
    "        return x    \n",
    "    \n",
    "fpex4 = FinalPatchExpand(dim=96, dim_scale=3).cuda()\n",
    "out_l = nn.Conv2d(in_channels=96,out_channels=2,kernel_size=1,bias=False).cuda()\n",
    "out_l = nn.Conv2d(in_channels=16,out_channels=1,kernel_size=1,bias=False).cuda()\n",
    "inp4 = torch.randn(16,16,64,96*3).cuda()\n",
    "out4 = inp4\n",
    "# out4 = fpex4(inp4)\n",
    "out4 = out_l(out4)\n",
    "print(f\"out4.shape = {out4.shape}\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3a3619f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def s_flatten(x):\n",
    "    B, T, H, W, C = x.shape\n",
    "    x = x.view(B, T, H*W, C)\n",
    "    return x\n",
    "\n",
    "def s_unflatten(x, H, W):\n",
    "    B, T, HW, C = x.shape\n",
    "    x = x.view(B, T, H, W, C)\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "137460a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x.shape after condenseT: torch.Size([16, 1, 64, 96])\n",
      "out4v.shape = torch.Size([16, 24, 24, 96])\n"
     ]
    }
   ],
   "source": [
    "class FinalPatchExpandVideo(nn.Module):\n",
    "    \"\"\"\n",
    "    FinalPatchExpand layer.\n",
    "    # a kind of unembedding\n",
    "    Upsamples the input feature map by a factor of 4 by converting channel\n",
    "    information into spatial resolution. The operation applies a linear\n",
    "    projection followed by a PixelShuffle-style rearrangement:\n",
    "    (B, H, W, C) → (B, 4H, 4W, C).\n",
    "    \"\"\"\n",
    "    def __init__(self, T, dim, dim_scale=3, norm_layer=nn.LayerNorm):\n",
    "        super().__init__()\n",
    "        self.dim = dim\n",
    "        self.dim_scale = dim_scale\n",
    "        self.expand = nn.Linear(dim, self.dim_scale**2 * dim, bias = False)\n",
    "            # applied to last dimension\n",
    "        self.output_dim = dim            \n",
    "        self.norm = norm_layer(self.output_dim)\n",
    "        self.condenseT = nn.Conv2d(in_channels=T, out_channels=1, kernel_size=1, bias=False)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.condenseT(x)\n",
    "        print(f\"x.shape after condenseT: {x.shape}\")\n",
    "        x = s_unflatten(x, H=8, W=8).squeeze(1)\n",
    "        x = self.expand(x)\n",
    "        B, H, W, C = x.shape\n",
    "        x = rearrange(x, 'b h w (p1 p2 c) -> b (h p1) (w p2) c', p1=self.dim_scale, p2=self.dim_scale, c=C//self.dim_scale**2)\n",
    "        x = self.norm(x)\n",
    "        return x    \n",
    " \n",
    "fpex4v = FinalPatchExpandVideo(dim=96, T = 16, dim_scale=3).cuda()\n",
    "inp4v = torch.randn(16,16,64,96).cuda()\n",
    "out4v = fpex4v(inp4v)\n",
    "print(f\"out4v.shape = {out4v.shape}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
